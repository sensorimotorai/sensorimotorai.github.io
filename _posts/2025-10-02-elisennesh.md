---
title: "RL Debates 1: Eli <em>\"abolish the value function\"</em> Sennesh"
layout: single
tags: [active-inference, value-function, optimal-control, allostasis]
categories: [2025, October]
---

In our 3rd meeting (the 1st in the [RL Debate Series]({{ "/debates/" | relative_url }})), we kicked off a contentious discussion on the role of the value function in RL, and why Eli believes it should be abolished.

- Paper: [Interoception as modeling, allostasis as control](https://www.sciencedirect.com/science/article/abs/pii/S0301051121002350){:target="_blank"}
- Slides: [Drive link](https://drive.google.com/file/d/1wTbQW6BxwcsxuxtLvrinK_4XZYt8Xzti/view?usp=sharing){:target="_blank"}
- Presenter: Eli Sennesh

We began with a roundtable of introductions from the debate's contenders, each giving a brief overview of their position. Eli Sennesh started by introducing the provocative phrase he coined, *"abolish the value function"*, [00:56] and arguing for a more complete understanding of the brain that separates decision-making and control problems [02:14]. The discussion also touched on the limitations of reward being an environmental given, with Eli pointing out that to model animal behavior accurately, the internal state of the agent must be considered [39:46]. He proposed that instead of maximizing a "substance" like rewards, we should think in terms of minimizing the "distance" to an optimal trajectory [01:02:49].

Watch the full meeting here:

<iframe width="560" height="315" src="https://www.youtube.com/embed/E0A0v53SeQU?si=UfD5_BGD8B8-TFNP" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>